{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "All imports that we need for this ML model. Model we will build: linear regression. Linear regression basically draws a line in an X dimensional graph, where X>1 & X is a natural number, that shows the trend of the graph. In such cases if you have all values but 1 you are able to find that value by using the slope and the bias of the graph. ( eg.: y=mx*b )."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First lets import the data set. Using titanic data set where we have to determine who will most likely survive based on the information given."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') #training\n",
    "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') #evaluation\n",
    "\n",
    "y_train = dftrain.pop('survived') #removes the survived column from the loaded csv and stores it in the variable\n",
    "y_eval = dfeval.pop('survived') #same as the above but for the second dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we have to get our data ready. As some data has string values we have to parse it into our feature columns ( the data we will feed the model ) as numerical data. To do that we use tensorflow categorical_column_with_vocabulary_list which takes in a vocabulary ( each different feature name will get a unique numerical id ) and the feature name to encode.\n",
    "\n",
    "Data values that are already represented by numbers are simply appended as float32's."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['sex','n_siblings_spouses','parch','class','deck','embark_town','alone']\n",
    "NUMERIC_COLUMNS = ['age','fare']\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    vocab = dftrain[feature_name].unique()\n",
    "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocab))\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have to prepare our data for the model. We need for that a tf.data.Dataset object. We define a function that takes in the data for training, the results expected for the training, the number of epochs ( how many times will the ML model learn from the same data set ), if we should or not shuffle the dataset for each epoch ( helps training our model to think instead of just memorizing ), and the batch size ( this helps with enormous amounts of data, here is kinda pointless, but it's good to know ).\n",
    "\n",
    "The first function returns a second inner function ( we get our object this way ). In the inner function we create the tf Dataset, shuffle it if necessary, then split it into batches.\n",
    "\n",
    "Important: evaluation data should be only 1 epoch ( pointless having more )."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
    "    def input_function():  # inner function, this will be returned\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000)  # randomize order of data\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs\n",
    "        return ds  # return a batch of the dataset\n",
    "    return input_function  # return a function object for use\n",
    "\n",
    "train_input_fn = make_input_fn(dftrain, y_train, shuffle=True)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time to start making the ML model.\n",
    "\n",
    "We first pass the before created feature_columns to a LinearClassifier from the estimator module in tensorflow. Estimators are basic implementations of algorithms in tensorflow. This will basically create our model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_est = tf.estimator.LinearClassifier(feature_columns);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TRAINING!!\n",
    "\n",
    "Here is where tensorflow makes its magic, and we just sit and watch like a true overlord ( see code comments to see how it works ). It takes a while to train and evaluate so just do something else until it's done."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "linear_est.train(train_input_fn) #here we tell tensorflow to train the model using the batched data\n",
    "result = linear_est.evaluate(eval_input_fn) #we evaluate the model after it has been trained and get metrics/stats\n",
    "\n",
    "clear_output() #we clear the console to get rid of the spam\n",
    "print(result) #we print the accuracy stats of the model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We trained & evaluated our model. Time to predict!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = list(linear_est.predict(eval_input_fn))\n",
    "clear_output()\n",
    "print(dfeval.loc[4], result[4]['probabilities'][1], y_eval.loc[4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it for lineal regression."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
